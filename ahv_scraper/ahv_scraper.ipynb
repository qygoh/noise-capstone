{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T22:47:25.593159Z",
     "start_time": "2019-07-01T22:47:24.759739Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T00:11:21.151835Z",
     "start_time": "2019-07-02T00:11:21.124957Z"
    }
   },
   "outputs": [],
   "source": [
    "def main(start_ahv_id='00895134', \n",
    "         not_found_retries=5, \n",
    "         max_times_blocked=0,\n",
    "         max_times_hard_blocked=5,\n",
    "         sleep_seconds=1,\n",
    "         spoof_chrome=True,\n",
    "         show_print_log=True,\n",
    "         csv_filename='AHV Data.csv',\n",
    "         csv_write_mode='append',\n",
    "         delete_terminal_not_found_rows_and_infer_start=True):\n",
    "    \"\"\"\n",
    "    Scrape the DOB's website for the latest AHVs and write selected attributes to CSV.\n",
    "    \n",
    "    Arguments:\n",
    "        start_ahv_id: The AHV ID from which to begin scraping (with or without two leading zeros)\n",
    "        not_found_retries: Threshold for when to stop the scraping based on consecutive not found results\n",
    "        max_times_blocked: Threshold for then to stop the scraping based on blocks (0=no limit)\n",
    "        sleep_seconds: Seconds to sleep between requests\n",
    "        spoof_chrome: Use a user agent and request headers from Google Chrome (may reduce blocks?)\n",
    "        show_print_log: Print each AHV and its status during run\n",
    "        csv_filename: CSV file to read and write to\n",
    "        csv_write_mode: 'overwrite' to recreate the file; append to add the latest AHV rows\n",
    "        delete_terminal_not_found_rows_and_infer_start: If true, the function will delete the last rows\n",
    "            with a not found status and resume scraping from the latest valid AHV found\n",
    "        \n",
    "    Returns:\n",
    "        A CSV file (based on csv_filename argument) with the AHV attributes\n",
    "    \"\"\"\n",
    "\n",
    "    if csv_write_mode == 'append' and delete_terminal_not_found_rows_and_infer_start:\n",
    "        df = pd.read_csv(csv_filename, index_col=0)\n",
    "        highest_ahv_id = df[df['status']!='AHV does not exist'].index.max()\n",
    "        df[df.index<=highest_ahv_id].to_csv(csv_filename)  \n",
    "        start_ahv_id = int(highest_ahv_id) + 1\n",
    "\n",
    "    current_ahv_id = int(start_ahv_id)\n",
    "    url = 'http://a810-bisweb.nyc.gov/bisweb/AHVPermitDetailsServlet'\n",
    "    blocked_counter = 0\n",
    "    not_found_counter = 0\n",
    "    hard_blocked_counter = 0\n",
    "\n",
    "    headers = {\n",
    "        'Accept': \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3\"\n",
    "        }\n",
    "    if spoof_chrome:\n",
    "        headers = {\n",
    "            'Accept': \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3\",\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Accept-Language': 'en-US,en;q=0.9',\n",
    "            'Cache-Control': 'max-age=0',\n",
    "            'Connection': 'keep-alive',\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36'\n",
    "            }\n",
    "\n",
    "    blocked_counter_limit = 999999 if max_times_blocked == 0 else max_times_blocked\n",
    "    \n",
    "    hard_blocked_counter_limit = 999999 if max_times_hard_blocked == 0 else max_times_hard_blocked\n",
    "\n",
    "    file_mode = 'w' if csv_write_mode == 'overwrite' else 'a'\n",
    "\n",
    "    with open(csv_filename, file_mode) as csvfile:\n",
    "\n",
    "        writer = csv.writer(csvfile)\n",
    "\n",
    "        if file_mode == 'w':\n",
    "            writer.writerow(['ahv_id', \n",
    "                            'timestamp_utc', \n",
    "                            'building_identification_number', \n",
    "                            'status',\n",
    "                            'residence_within_200_feet',\n",
    "                            'enclosed_building_work',\n",
    "                            'full_or_partial_demolition',\n",
    "                            'crane_use',\n",
    "                            'requested_for_date_ranges',\n",
    "                            'apply_reason',\n",
    "                            'work_description'\n",
    "                            ])\n",
    "\n",
    "        while not_found_counter < not_found_retries \\\n",
    "            and blocked_counter < blocked_counter_limit \\\n",
    "            and hard_blocked_counter < hard_blocked_counter_limit:\n",
    "\n",
    "            # Set up call and parse HTML\n",
    "            current_ahv_str = '00' + str(current_ahv_id)\n",
    "            \n",
    "            if show_print_log:\n",
    "                print(\"AHV ID:\", current_ahv_str)\n",
    "\n",
    "            response = requests.request(\"GET\", url, headers=headers, params={\"allkey\":current_ahv_str})\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Server/client error\n",
    "            if response.status_code >= 400:\n",
    "                if show_print_log:\n",
    "                    print(\"Error status code:\", response.status_code)\n",
    "            # Request blocked due to high demand\n",
    "            if soup.title.string == 'Visitor Prioritization - NYC Department of Buildings':\n",
    "                blocked_counter += 1\n",
    "                \n",
    "                if show_print_log:\n",
    "                    print(\"Blocked\")\n",
    "\n",
    "            # ID does not have an AHV\n",
    "            elif soup.find('td', {'class':'errormsg'}):\n",
    "                writer.writerow(\n",
    "                    [current_ahv_str,\n",
    "                     datetime.datetime.utcnow().isoformat(),\n",
    "                     '',\n",
    "                     'AHV does not exist',\n",
    "                     '',\n",
    "                     '',\n",
    "                     '',\n",
    "                     '',\n",
    "                     '',\n",
    "                     '',\n",
    "                     ''\n",
    "                    ])\n",
    "\n",
    "                if show_print_log:\n",
    "                    print(\"Not found\")\n",
    "                not_found_counter += 1\n",
    "                current_ahv_id += 1\n",
    "\n",
    "            # AHV found\n",
    "            elif soup.findAll('td', {\"class\": \"maininfo\"}):\n",
    "\n",
    "                ahv_id = current_ahv_str\n",
    "                timestamp_utc = datetime.datetime.utcnow().isoformat()\n",
    "                building_identification_number = soup.findAll('td', {\"class\": \"maininfo\", \"colspan\": \"3\"})[-1].a.string\n",
    "                status = soup.findAll('td', {\"class\": \"content\", \"colspan\":\"4\"})[0].string\n",
    "\n",
    "                bools = ['_check' in str(i) for i in soup.findAll('img',{'height':10})[::2]]\n",
    "                residence_within_200_feet, enclosed_building_work, full_or_partial_demolition, crane_use = bools\n",
    "\n",
    "                start_days = [i.string for i in soup.findAll('td', {\"class\": \"centercontent\"})[5:][::4]]\n",
    "                start_times = [i.string for i in soup.findAll('td', {\"class\": \"centercontent\"})[7:][::4]]\n",
    "                end_times = [i.string for i in soup.findAll('td', {\"class\": \"centercontent\"})[8:][::4]]\n",
    "                requested_for_date_ranges = json.dumps(dict(zip(start_days,(zip(start_times, end_times)))))\n",
    "                apply_reason = soup.findAll('td', {'class':'content', 'colspan':7})[-2].text.split('\\xa0')[2]\n",
    "                work_description = soup.find('td', {'class':'content','colspan':6,'valign':'top'}).string\n",
    "\n",
    "                writer.writerow(\n",
    "                    [ahv_id,\n",
    "                     timestamp_utc,\n",
    "                     building_identification_number,\n",
    "                     status,\n",
    "                     residence_within_200_feet,\n",
    "                     enclosed_building_work,\n",
    "                     full_or_partial_demolition,\n",
    "                     crane_use,\n",
    "                     requested_for_date_ranges,\n",
    "                     apply_reason,\n",
    "                     work_description,\n",
    "                    ])\n",
    "\n",
    "                if show_print_log:\n",
    "                    print(\"Scraped\")\n",
    "                current_ahv_id += 1\n",
    "                not_found_counter = 0\n",
    "                \n",
    "            time.sleep(sleep_seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T00:15:34.748226Z",
     "start_time": "2019-07-02T00:15:34.236862Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AHV ID: 00895404\n",
      "Not found\n",
      "AHV ID: 00895405\n",
      "Not found\n",
      "AHV ID: 00895406\n",
      "Not found\n",
      "AHV ID: 00895407\n",
      "Not found\n",
      "AHV ID: 00895408\n",
      "Not found\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "   main(start_ahv_id='00893000',\n",
    "        not_found_retries=5, \n",
    "        max_times_blocked=0, \n",
    "        max_times_hard_blocked=5,\n",
    "        sleep_seconds=0.05,\n",
    "        spoof_chrome=True,\n",
    "        show_print_log=True,\n",
    "        csv_filename='ahv_data_20190622_20190630.csv',\n",
    "        csv_write_mode='append',\n",
    "        delete_terminal_not_found_rows_and_infer_start=True\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T00:22:56.390237Z",
     "start_time": "2019-07-02T00:22:51.718872Z"
    }
   },
   "outputs": [],
   "source": [
    "df_address = pd.read_csv('Address_Point.csv')\n",
    "df_ahv = pd.read_csv('ahv_data_20190622_20190630.csv')\n",
    "df = df_ahv.merge(df_address[['the_geom','BIN','H_NO','FULL_STREE','ZIPCODE']], how='left', left_on = 'building_identification_number', right_on='BIN')\n",
    "df.drop(['BIN'], axis=1, inplace=True)\n",
    "df.rename(columns={\"H_NO\": \"house_number\", \"FULL_STREE\": \"street_name\", \"ZIPCODE\":\"zip_code\"}, inplace=True)\n",
    "df.to_csv('ahv_data_with_geo_20190622_20190630.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
