{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Predicting Noise (complaints)\n",
    "#### Author: Sung Hoon Yang @ NYU\n",
    "\n",
    "*Below is the abstract of the paper for which the prediction was conducted*\n",
    "\n",
    "Noise in New York City is increasingly unbearable as evidenced by a growing number of noise complaints, while responses to noise complaints and therefore enforcement of The Noise Code has been hampered due to inability to handle sheer volumes. This capstone project aims to provide a data-driven optimization approach to improve the New York City Department of Environmental Protectionâ€™s (DEP) current scheduling process to better address noise complaints. To accomplish this, we will use machine learning to predict noise complaints and qualities that lead to violations for scheduling and routing optimization. Based on our discussion with DEP sponsors and preliminary analysis of 311 complaints, we suspect that our model will improve DEP metrics and that construction-related permits will be of highest variable importance to predicting and determining the validity of noise complaints. The implications of this analysis will allow DEP inspectors to improve time to complaint resolution, vanquish their backlog of complaints, and increase their issuance of violations.\n",
    "\n",
    "Using publicly available data, we will build a neural network model to predict daily noise complaints per spatial bin as demarcated by Neighborhood Tabulation Areas. In this particular model, I will build a model that predicts the number of daily complaints that pertain to DEP's overseeing per 29 polygons of Manhattan. Below is the complete list of features, followed by that of spatial bins as demarcated by NTA shapefile.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Features\n",
    "* **wknd**: Weekend flag\n",
    "* **holiday**: Holiday flag\n",
    "* **hours_to_resolution_stdz**: Hours to Resolution standardized annually\n",
    "* **AWND**: Average daily wind speed (tenths of meters per second)\n",
    "* **PRCP**: Precipitation (tenths of mm) \n",
    "* **SNOW**: Snowfall (mm) \n",
    "* **SNWD**: Snow depth (mm) \n",
    "* **TMAX**: Maximum temperature (tenths of degrees C)\n",
    "* **WDF5**: Direction of fastest 5-second wind (degrees)\n",
    "* **WSF5**: Fastest 5-second wind speed (tenths of meters per second) \n",
    "* **d-1_cnt**: Complaint count of D-1\n",
    "* **d-2_cnt**: Complaint count of D-2\n",
    "* **d-3_cnt**: Complaint count of D-3\n",
    "* **d-4_cnt**: Complaint count of D-4\n",
    "* **d-5_cnt**: Complaint count of D-5\n",
    "* **d-6_cnt**: Complaint count of D-6\n",
    "* **d-7_cnt**: Complaint count of D-7\n",
    "* **d-8_cnt**: Complaint count of D-8\n",
    "* **d-9_cnt**: Complaint count of D-9\n",
    "* **d-10_cnt**: Complaint count of D-10\n",
    "* **d-11_cnt**: Complaint count of D-11\n",
    "* **d-12_cnt**: Complaint count of D-12\n",
    "* **d-13_cnt**: Complaint count of D-13\n",
    "* **d-14_cnt**: Complaint count of D-14\n",
    "* **d-15_cnt**: Complaint count of D-15\n",
    "* **d-16_cnt**: Complaint count of D-16\n",
    "* **d-17_cnt**: Complaint count of D-17\n",
    "* **d-18_cnt**: Complaint count of D-18\n",
    "* **d-19_cnt**: Complaint count of D-19\n",
    "* **d-20_cnt**: Complaint count of D-20\n",
    "* **d-21_cnt**: Complaint count of D-21\n",
    "* **d-22_cnt**: Complaint count of D-22\n",
    "* **d-23_cnt**: Complaint count of D-23\n",
    "* **d-24_cnt**: Complaint count of D-24\n",
    "* **d-25_cnt**: Complaint count of D-25\n",
    "* **d-26_cnt**: Complaint count of D-26\n",
    "* **d-27_cnt**: Complaint count of D-27\n",
    "* **d-28_cnt**: Complaint count of D-28\n",
    "* **ahv_open_cnt**: after hour variance open count\n",
    "* **WT01**: unknown weather feature (omitted from data dictionary, but included)\n",
    "* **WT02**: unknown weather feature (omitted from data dictionary, but included)\n",
    "* **WT04**: unknown weather feature (omitted from data dictionary, but included)\n",
    "* **WT08**: unknown weather feature (omitted from data dictionary, but included)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Spatial Bins\n",
    "\n",
    "* **MN01**:\tMarble Hill-Inwood\n",
    "* **MN03**:\tCentral Harlem North-Polo Grounds\n",
    "* **MN04**:\tHamilton Heights\n",
    "* **MN06**:\tManhattanville\n",
    "* **MN09**:\tMorningside Heights\n",
    "* **MN11**:\tCentral Harlem South\n",
    "* **MN12**:\tUpper West Side\n",
    "* **MN13**:\tHudson Yards-Chelsea-Flatiron-Union Square\n",
    "* **MN14**:\tLincoln Square\n",
    "* **MN15**:\tClinton\n",
    "* **MN17**:\tMidtown-Midtown South\n",
    "* **MN19**:\tTurtle Bay-East Midtown\n",
    "* **MN20**:\tMurray Hill-Kips Bay\n",
    "* **MN21**:\tGramercy\n",
    "* **MN22**:\tEast Village\n",
    "* **MN23**:\tWest Village\n",
    "* **MN24**:\tSoHo-TriBeCa-Civic Center-Little Italy\n",
    "* **MN25**:\tBattery Park City-Lower Manhattan\n",
    "* **MN27**:\tChinatown\n",
    "* **MN28**:\tLower East Side\n",
    "* **MN31**:\tLenox Hill-Roosevelt Island\n",
    "* **MN32**:\tYorkville\n",
    "* **MN33**:\tEast Harlem South\n",
    "* **MN34**:\tEast Harlem North\n",
    "* **MN35**:\tWashington Heights North\n",
    "* **MN36**:\tWashington Heights South\n",
    "* **MN40**:\tUpper East Side-Carnegie Hill\n",
    "* **MN50**:\tStuyvesant Town-Cooper Village\n",
    "* **MN99**:\tpark-cemetery-etc-Manhattan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#torch\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if device == 'cuda':\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "import gc\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import ConcatDataset\n",
    "#wrangling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from itertools import product\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy\n",
    "#dt\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "#user setting\n",
    "import sys\n",
    "sys.path.insert(0, './analysis/311/duke')\n",
    "# precipitation data\n",
    "from prep_dta import _2010, _2011, _2012, _2013, _2014, _2015, _2016, _2017, _2018\n",
    "from american_holidays import american_holidays as _american_holidays_str\n",
    "#viz\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "spatial_dict = {'MN01':'Marble Hill-Inwood','MN03':'Central Harlem North-Polo Grounds','MN04':'Hamilton Heights','MN06':'Manhattanville','MN09':'Morningside Heights','MN11':'Central Harlem South','MN12':'Upper West Side','MN13':'Hudson Yards-Chelsea-Flatiron-Union Square','MN14':'Lincoln Square','MN15':'Clinton','MN17':'Midtown-Midtown South','MN19':'Turtle Bay-East Midtown','MN20':'Murray Hill-Kips Bay','MN21':'Gramercy','MN22':'East Village','MN23':'West Village','MN24':'SoHo-TriBeCa-Civic Center-Little Italy','MN25':'Battery Park City-Lower Manhattan','MN27':'Chinatown','MN28':'Lower East Side','MN31':'Lenox Hill-Roosevelt Island','MN32':'Yorkville','MN33':'East Harlem South','MN34':'East Harlem North','MN35':'Washington Heights North','MN36':'Washington Heights South','MN40':'Upper East Side-Carnegie Hill','MN50':'Stuyvesant Town-Cooper Village','MN99':'park-cemetery-etc-Manhattan',}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "features_stdz = pd.read_csv('./features_stdz.csv')\n",
    "features_stdz.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "targets_cpu = np.loadtxt('./targets.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "targets_cpu[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Inspect Hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "I am using MSELoss. Below shows that covariance matrix is essentially the Hessian of the Loss function. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{MSE} &= \\frac{1}{N}||W\\vec{x} - \\vec{y}||^2 \\quad \\hat{y} :=W\\vec{x}\\\\\n",
    "\\nabla_{W} \\text{MSE} &= \\frac{2}{N} ||W\\vec{x} - \\vec{y}||\\vec{x}^T \\\\\n",
    "\\nabla_{W}^2 \\text{MSE} &= \\frac{1}{N} 2\\vec{x}\\vec{x}^T \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Let us examine the covariance matrix of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.clf()\n",
    "plt.imshow(features_stdz.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If the covariance matrix diverges from Identity, some eigenvectors may have a small magnitude, collapsing the corresponding axis of the Loss space. But, it's okay if you use `SGD` with a small learning rate, because you cannot examine the entire Hypothesis space. If repeated predictions result in similar minimization, we can be confident that it is close tothe global minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Measure sizes\n",
    "features = np.array(features_stdz)\n",
    "S = 29\n",
    "T = int(features.shape[0] / S)\n",
    "Fe=features.shape[1]\n",
    "H=Fe*S//4\n",
    "batch_size=1\n",
    "num_epochs = 40\n",
    "\n",
    "features_nn = features.reshape(T, S*Fe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features_nn\n",
    "    , targets_cpu\n",
    "    , test_size=0.2\n",
    "    , shuffle=False\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dev_cnt = torch.cuda.device_count()\n",
    "\n",
    "train_ds = DataLoader(\n",
    "    torch.from_numpy(np.concatenate((X_train, y_train), axis=1))\n",
    "    , batch_size=batch_size\n",
    "    , drop_last=True\n",
    "    , shuffle=True\n",
    "    , num_workers=dev_cnt*6\n",
    "    , pin_memory=True\n",
    ")\n",
    "\n",
    "test_ds = DataLoader(\n",
    "    torch.from_numpy(np.concatenate((X_test, y_test), axis=1))\n",
    "    , batch_size=batch_size\n",
    "    , drop_last=True    \n",
    "    , num_workers=dev_cnt*6\n",
    "    , pin_memory=True    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Here we define our model as a class\n",
    "class ManhattanModel(nn.Module):\n",
    "    def __init__(self, input_dim, feature_dim, hidden_dim, output_dim, num_layers, batch_size):\n",
    "        \"\"\"\n",
    "        The model uses LSTM model as both Encoder/Decoder for this undercomplete Autoencoder model.\n",
    "        * Batch normalization is used for all linear layers.\n",
    "        * The autoencoder compresses the representation to hidden_dim/4, \n",
    "            and then recovers the dimensionality back to hidden_dim\n",
    "        * Softmax layer is used to output pseudo-probability density of complaint volume \n",
    "            of each spatial bin on each day.\n",
    "        \"\"\"\n",
    "        super(ManhattanModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.feature_dim = feature_dim        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim        \n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm1 = nn.LSTM(\n",
    "            input_dim*feature_dim\n",
    "            , self.hidden_dim\n",
    "            , self.num_layers\n",
    "            , dropout=0.1\n",
    "            , batch_first=True\n",
    "        )\n",
    "        self.batchnorm1d_1 = nn.BatchNorm1d(batch_size)            \n",
    "        self.linear_1 = nn.Linear(self.hidden_dim, int(self.hidden_dim/2))\n",
    "        self.batchnorm1d_2 = nn.BatchNorm1d(batch_size)   \n",
    "        self.linear_2 = nn.Linear(int(self.hidden_dim/2), int(self.hidden_dim/4))\n",
    "        self.lstm2 = nn.LSTM(\n",
    "            int(self.hidden_dim/4)\n",
    "            , self.hidden_dim\n",
    "            , self.num_layers\n",
    "            , dropout=0.1\n",
    "            , batch_first=True\n",
    "        )\n",
    "        self.batchnorm1d_3 = nn.BatchNorm1d(batch_size)    \n",
    "        self.linear_3 = nn.Linear(self.hidden_dim, int(self.hidden_dim/2))\n",
    "        self.batchnorm1d_4 = nn.BatchNorm1d(batch_size)    \n",
    "        self.linear_4 = nn.Linear(int(self.hidden_dim/2), output_dim)        \n",
    "        \n",
    "    def init_hidden(self):\n",
    "        # This is what we'll initialise our hidden state as\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "\n",
    "    def forward(self, x, h1, h2):\n",
    "        if h1 is None:\n",
    "            x, h1 = self.lstm1(x)\n",
    "        else:\n",
    "            x, h1 = self.lstm1(x, h1)\n",
    "        x = self.batchnorm1d_1(x)\n",
    "        x = self.linear_1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.batchnorm1d_2(x)\n",
    "        x = self.linear_2(x)   \n",
    "        x = F.relu(x)        \n",
    "        if h2 is None:\n",
    "            x, h2 = self.lstm2(x)\n",
    "        else:\n",
    "            x, h2 = self.lstm2(x, h2)\n",
    "        x = self.linear_3(x)  \n",
    "        x = self.batchnorm1d_3(x)\n",
    "        x = F.relu(x)        \n",
    "        x = self.batchnorm1d_4(x)\n",
    "        x = self.linear_4(x)  \n",
    "        x = F.relu(x)  \n",
    "        x = F.softmax(x, dim=-1)\n",
    "        return x, h1, h2\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model_s = ManhattanModel(S, Fe, H, S, 1, batch_size)\n",
    "# model_s = ManhattanDenseNet(S, Fe, H, S, 2, batch_size)\n",
    "\n",
    "model = nn.DataParallel(model_s, device_ids=range(dev_cnt)) #I want to use all available GPUs anyhow\n",
    "# model = model.cuda()\n",
    "model.to(device)\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=8, gamma=0.1)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "losses = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "d = torch.load('/scratch/shy256/capstone/cs_lstm_L1_AEDE_SM.pth')\n",
    "\n",
    "model.load_state_dict(d)\n",
    "\n",
    "model.eval()\n",
    "losses = []\n",
    "preds = []\n",
    "targets = []\n",
    "c_1, c_2 = None, None\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, data in enumerate(test_ds):\n",
    "        X_test_bf = data[:, :Fe*S].reshape(1, batch_size, Fe*S).type(torch.cuda.FloatTensor)\n",
    "        y_test_bf = data[:, Fe*S:].type(torch.cuda.FloatTensor)\n",
    "        output, c_1, c_2 = model(X_test_bf, c_1, c_2)\n",
    "        collapsed_size = torch.tensor(y_test_bf.shape).cumprod(dim=0)[-1].item()\n",
    "        pred = output.view(collapsed_size)\n",
    "        preds.extend(pred)\n",
    "        target = F.softmax(y_test_bf.view(collapsed_size))\n",
    "        targets.extend(target)        \n",
    "        loss = loss_fn(\n",
    "            pred.squeeze()\n",
    "            , target.squeeze()\n",
    "        )\n",
    "        losses.append(loss.item())\n",
    "        loss_t = np.mean(losses[-1:])\n",
    "#         print(\n",
    "#             '[test] [b]:%s - [loss]:%s' \\\n",
    "#             % (batch_idx, str(loss_t))\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "target_vals_orig = np.array(targets).reshape(len(targets) // 29, 29)\n",
    "pred_vals_orig = np.array(preds).reshape(len(targets) // 29, 29)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "target_vals = target_vals_orig.copy()\n",
    "pred_vals = pred_vals_orig.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "plt.clf()\n",
    "dts = pd.date_range(\n",
    "    datetime.datetime.strptime(\"2017-08-27\", '%Y-%m-%d') - relativedelta(days=len(losses) - 1)\n",
    "    , datetime.datetime.strptime(\"2017-08-27\", '%Y-%m-%d')\n",
    ")\n",
    "dts_str = [datetime.datetime.strftime(dt, '%Y-%m-%d') for dt in dts]\n",
    "mn_nms = [\"MN14\" , \"MN31\" , \"MN32\" , \"MN12\" , \"MN20\" , \"MN23\" , \"MN24\" , \"MN01\" , \"MN03\" , \"MN09\" , \"MN13\" , \"MN22\" , \"MN25\" , \"MN27\" , \"MN36\" , \"MN17\" , \"MN19\" , \"MN21\" , \"MN28\" , \"MN33\" , \"MN15\" , \"MN35\" , \"MN40\" , \"MN11\" , \"MN04\" , \"MN34\" , \"MN06\" , \"MN99\" , \"MN50\"]\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    S * 2 + 2, 1, figsize=(16, 80)\n",
    ")\n",
    "# plt.subplots_adjust(bottom=0.9, top=1)\n",
    "axs[0].plot(dts, target_vals.sum(axis=1), label='target')\n",
    "axs[0].plot(dts, pred_vals.sum(axis=1), label='prediction')\n",
    "tot_avg = target_vals.sum(axis=1).astype(np.float64).mean()\n",
    "tot_std = target_vals.sum(axis=1).astype(np.float64).std()\n",
    "axs[0].set_ylabel(f'Total\\nm:{tot_avg:.1f}\\nstdev:{tot_std:.1f}')\n",
    "# axs[1].plot(dts, losses)\n",
    "\n",
    "tot_acc = np.array(torch.Tensor((pred_vals.sum(axis=1) / target_vals.sum(axis=1)).astype(np.float64)).cpu())\n",
    "axs[1].plot(\n",
    "    dts, tot_acc * 100, color='g', label='score'    \n",
    ")\n",
    "axs[1].set_ylim(0, 100)\n",
    "axs[1].set_ylabel('Score') \n",
    "axs[1].bar(dts, tot_acc * 100, 1.1, color='g')\n",
    "axs[1].bar(dts, 100-tot_acc*100, 1.1, bottom=tot_acc * 100, color='r')\n",
    "\n",
    "losses = np.array(losses)\n",
    "loss_avg = losses.mean()\n",
    "loss_std = losses.std()\n",
    "axs[1].set_ylabel('Score')\n",
    "\n",
    "for i in range(0, S):\n",
    "    if i == 15:\n",
    "        break\n",
    "    # replace zero with the mean to make the prediction accuracy graph look better\n",
    "    index_both_zero = list(set(np.where(target_vals[:,i].astype(np.float64) == 0)[0]) & set(np.where(pred_vals[:,i].astype(np.float64) < 5e-1)[0]))\n",
    "    np.put(target_vals[:,i], index_both_zero, [1] * len(index_both_zero))\n",
    "    np.put(pred_vals[:,i], index_both_zero, [1] * len(index_both_zero))\n",
    "    # if target of pred is zero and the other is not, assign 0.5 to coerce to accuracy\n",
    "    target_vals[:,i][target_vals[:,i] <= 8e-1] = 1e-5\n",
    "    pred_vals[:,i][pred_vals[:,i] <= 8e-1] = 1e-5\n",
    "    denom = np.max(np.array([target_vals[:,i], pred_vals[:,i]]), axis=0)\n",
    "    numer = np.min(np.array([target_vals[:,i], pred_vals[:,i]]), axis=0)\n",
    "    acc = np.array(torch.Tensor((numer / denom).astype(np.float64)).cpu())\n",
    "    t, p, nm = target_vals[:,i].astype(np.float64), pred_vals[:,i].astype(np.float64), mn_nms[i]\n",
    "    i *= 2\n",
    "    i += 2    \n",
    "    axs[i].plot(\n",
    "        dts, t, 'r-'\n",
    "        , dts, p, 'b-'\n",
    "    )\n",
    "    cnt_avg = t.mean()\n",
    "    pred_cnt_avg = p.mean()    \n",
    "    cnt_std = t.std()    \n",
    "    pred_cnt_std = p.std()       \n",
    "    axs[i].set_ylabel(spatial_dict[nm].replace(' ', '\\n') + f'\\nm:{cnt_avg:.1f}/{pred_cnt_avg:.1f}\\nstdev:{cnt_std:.1f}/{pred_cnt_std:.1f}')\n",
    "    acc[~np.isfinite(acc)] = 0\n",
    "    axs[i + 1].plot(\n",
    "        dts, acc * 100, color='g', label='score'    \n",
    "    )\n",
    "    axs[i + 1].set_ylim(0, 100)\n",
    "    axs[i + 1].set_ylabel('Score') \n",
    "    axs[i + 1].bar(dts, acc * 100, 1.1, color='g')\n",
    "    axs[i + 1].bar(dts, 100-acc*100, 1.1, bottom=acc * 100, color='r')\n",
    "plt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.25,\n",
    "                    wspace=0.05)\n",
    "plt.savefig('./test_result_b1-shuf_manhattan_dn.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "target_vals = target_vals_orig.copy()\n",
    "pred_vals = pred_vals_orig.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# see if I can do better by using the aggregate ranking\n",
    "static_ranking = np.flip(\n",
    "    np.array(list([spatial_dict[k] for k in mn_nms]))[np.argsort(target_vals.sum(axis=0).astype(np.float64))], 0)[:15]\n",
    "static_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Check the frequentists of the Model's Dynamic Ranking\n",
    "a = []\n",
    "[a.extend(list(frozenset(np.flip(np.array(list([spatial_dict[k] for k in mn_nms]))[np.argsort(target_vals[II].astype(np.float64))], 0)[:15]))) for II in range(0, 200)]\n",
    "\n",
    "pd.Series(a).unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "A = []\n",
    "static_ranking_arr = []\n",
    "for i in range(406):\n",
    "    v = len(list(set(np.flip(np.array(list([spatial_dict[k] for k in mn_nms]))[np.argsort(target_vals[i].astype(np.float64))], 0)[:15]) & \\\n",
    "set(np.flip(np.array(list([spatial_dict[k] for k in mn_nms]))[np.argsort(pred_vals[i].astype(np.float64))], 0)[:15]))) / 15\n",
    "    A.append(v)\n",
    "    v = len(list(set(np.flip(np.array(list(spatial_dict.values()))[np.argsort(target_vals[i].astype(np.float64))], 0)[:15]) & \\\n",
    "set(static_ranking))) / 15\n",
    "    static_ranking_arr.append(v)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 4))\n",
    "A = np.array(A)\n",
    "axs[0].plot(A, color='#afeeee')\n",
    "model_mean = np.array(A).mean()\n",
    "axs[0].set_title(f'Prediction with Model\\nmean accuracy: {model_mean:.3f}')\n",
    "axs[0].set_ylim(0, 1)\n",
    "axs[0].set_xlabel('Days')\n",
    "axs[0].set_ylabel('Score')\n",
    "axs[0].bar(np.arange(A.size), A, 1.1, color='#afeeee')\n",
    "axs[0].bar(np.arange(A.size), 1-A, 1.1, bottom=A, color='magenta')\n",
    "\n",
    "\n",
    "\n",
    "static_ranking_arr = np.array(static_ranking_arr)\n",
    "axs[1].plot(static_ranking_arr, color='#afeeee')\n",
    "static_ranking_mean = np.array(static_ranking_arr).mean()\n",
    "axs[1].set_title(f'Prediction with Static Ranking\\nmean accuracy: {static_ranking_mean:.3f}')\n",
    "axs[1].set_ylim(0, 1)\n",
    "axs[1].set_xlabel('Days')\n",
    "axs[1].set_ylabel('Score')\n",
    "axs[1].bar(np.arange(static_ranking_arr.size), static_ranking_arr, 1.1, color='#afeeee')\n",
    "axs[1].bar(np.arange(static_ranking_arr.size), 1-static_ranking_arr, 1.1, bottom=static_ranking_arr, color='magenta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Bibliography\n",
    "\n",
    "Menne, M.J., I. Durre, R.S. Vose, B.E. Gleason, and T.G. Houston, 2012:  An overview \n",
    "of the Global Historical Climatology Network-Daily Database.  Journal of Atmospheric \n",
    "and Oceanic Technology, 29, 897-910, doi:10.1175/JTECH-D-11-00103.1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpc-PY363",
   "language": "python",
   "name": "py3.6.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
