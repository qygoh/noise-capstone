{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Predicting Noise (complaints)\n",
    "#### Author: Sung Hoon Yang @ NYU\n",
    "\n",
    "*Below is the abstract of the paper for which the prediction was conducted*\n",
    "\n",
    "Noise in New York City is increasingly unbearable as evidenced by a growing number of noise complaints, while responses to noise complaints and therefore enforcement of The Noise Code has been hampered due to inability to handle sheer volumes. This capstone project aims to provide a data-driven optimization approach to improve the New York City Department of Environmental Protectionâ€™s (DEP) current scheduling process to better address noise complaints. To accomplish this, we will use machine learning to predict noise complaints and qualities that lead to violations for scheduling and routing optimization. Based on our discussion with DEP sponsors and preliminary analysis of 311 complaints, we suspect that our model will improve DEP metrics and that construction-related permits will be of highest variable importance to predicting and determining the validity of noise complaints. The implications of this analysis will allow DEP inspectors to improve time to complaint resolution, vanquish their backlog of complaints, and increase their issuance of violations.\n",
    "\n",
    "Using publicly available data, we will build a neural network model to predict daily noise complaints per spatial bin as demarcated by Neighborhood Tabulation Areas. In this particular model, I will build a model that predicts the number of daily complaints that pertain to DEP's overseeing per 29 polygons of Manhattan. Below is the complete list of features, followed by that of spatial bins as demarcated by NTA shapefile.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Features\n",
    "* **wknd**: Weekend flag\n",
    "* **holiday**: Holiday flag\n",
    "* **hours_to_resolution_stdz**: Hours to Resolution standardized annually\n",
    "* **AWND**: Average daily wind speed (tenths of meters per second)\n",
    "* **PRCP**: Precipitation (tenths of mm) \n",
    "* **SNOW**: Snowfall (mm) \n",
    "* **SNWD**: Snow depth (mm) \n",
    "* **TMAX**: Maximum temperature (tenths of degrees C)\n",
    "* **WDF5**: Direction of fastest 5-second wind (degrees)\n",
    "* **WSF5**: Fastest 5-second wind speed (tenths of meters per second) \n",
    "* **d-1_cnt**: Complaint count of D-1\n",
    "* **d-2_cnt**: Complaint count of D-2\n",
    "* **d-3_cnt**: Complaint count of D-3\n",
    "* **d-4_cnt**: Complaint count of D-4\n",
    "* **d-5_cnt**: Complaint count of D-5\n",
    "* **d-6_cnt**: Complaint count of D-6\n",
    "* **d-7_cnt**: Complaint count of D-7\n",
    "* **d-8_cnt**: Complaint count of D-8\n",
    "* **d-9_cnt**: Complaint count of D-9\n",
    "* **d-10_cnt**: Complaint count of D-10\n",
    "* **d-11_cnt**: Complaint count of D-11\n",
    "* **d-12_cnt**: Complaint count of D-12\n",
    "* **d-13_cnt**: Complaint count of D-13\n",
    "* **d-14_cnt**: Complaint count of D-14\n",
    "* **d-15_cnt**: Complaint count of D-15\n",
    "* **d-16_cnt**: Complaint count of D-16\n",
    "* **d-17_cnt**: Complaint count of D-17\n",
    "* **d-18_cnt**: Complaint count of D-18\n",
    "* **d-19_cnt**: Complaint count of D-19\n",
    "* **d-20_cnt**: Complaint count of D-20\n",
    "* **d-21_cnt**: Complaint count of D-21\n",
    "* **d-22_cnt**: Complaint count of D-22\n",
    "* **d-23_cnt**: Complaint count of D-23\n",
    "* **d-24_cnt**: Complaint count of D-24\n",
    "* **d-25_cnt**: Complaint count of D-25\n",
    "* **d-26_cnt**: Complaint count of D-26\n",
    "* **d-27_cnt**: Complaint count of D-27\n",
    "* **d-28_cnt**: Complaint count of D-28\n",
    "* **ahv_open_cnt**: after hour variance open count\n",
    "* **WT01**: unknown weather feature (omitted from data dictionary, but included)\n",
    "* **WT02**: unknown weather feature (omitted from data dictionary, but included)\n",
    "* **WT04**: unknown weather feature (omitted from data dictionary, but included)\n",
    "* **WT08**: unknown weather feature (omitted from data dictionary, but included)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Spatial Bins\n",
    "\n",
    "* **MN01**:\tMarble Hill-Inwood\n",
    "* **MN03**:\tCentral Harlem North-Polo Grounds\n",
    "* **MN04**:\tHamilton Heights\n",
    "* **MN06**:\tManhattanville\n",
    "* **MN09**:\tMorningside Heights\n",
    "* **MN11**:\tCentral Harlem South\n",
    "* **MN12**:\tUpper West Side\n",
    "* **MN13**:\tHudson Yards-Chelsea-Flatiron-Union Square\n",
    "* **MN14**:\tLincoln Square\n",
    "* **MN15**:\tClinton\n",
    "* **MN17**:\tMidtown-Midtown South\n",
    "* **MN19**:\tTurtle Bay-East Midtown\n",
    "* **MN20**:\tMurray Hill-Kips Bay\n",
    "* **MN21**:\tGramercy\n",
    "* **MN22**:\tEast Village\n",
    "* **MN23**:\tWest Village\n",
    "* **MN24**:\tSoHo-TriBeCa-Civic Center-Little Italy\n",
    "* **MN25**:\tBattery Park City-Lower Manhattan\n",
    "* **MN27**:\tChinatown\n",
    "* **MN28**:\tLower East Side\n",
    "* **MN31**:\tLenox Hill-Roosevelt Island\n",
    "* **MN32**:\tYorkville\n",
    "* **MN33**:\tEast Harlem South\n",
    "* **MN34**:\tEast Harlem North\n",
    "* **MN35**:\tWashington Heights North\n",
    "* **MN36**:\tWashington Heights South\n",
    "* **MN40**:\tUpper East Side-Carnegie Hill\n",
    "* **MN50**:\tStuyvesant Town-Cooper Village\n",
    "* **MN99**:\tpark-cemetery-etc-Manhattan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#torch\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if device == 'cuda':\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "import gc\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import ConcatDataset\n",
    "#wrangling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from itertools import product\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy\n",
    "#dt\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "#user setting\n",
    "import sys\n",
    "sys.path.insert(0, './analysis/311/duke')\n",
    "# precipitation data\n",
    "from prep_dta import _2010, _2011, _2012, _2013, _2014, _2015, _2016, _2017, _2018\n",
    "from american_holidays import american_holidays as _american_holidays_str\n",
    "#viz\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "spatial_dict = {'MN01':'Marble Hill-Inwood','MN03':'Central Harlem North-Polo Grounds','MN04':'Hamilton Heights','MN06':'Manhattanville','MN09':'Morningside Heights','MN11':'Central Harlem South','MN12':'Upper West Side','MN13':'Hudson Yards-Chelsea-Flatiron-Union Square','MN14':'Lincoln Square','MN15':'Clinton','MN17':'Midtown-Midtown South','MN19':'Turtle Bay-East Midtown','MN20':'Murray Hill-Kips Bay','MN21':'Gramercy','MN22':'East Village','MN23':'West Village','MN24':'SoHo-TriBeCa-Civic Center-Little Italy','MN25':'Battery Park City-Lower Manhattan','MN27':'Chinatown','MN28':'Lower East Side','MN31':'Lenox Hill-Roosevelt Island','MN32':'Yorkville','MN33':'East Harlem South','MN34':'East Harlem North','MN35':'Washington Heights North','MN36':'Washington Heights South','MN40':'Upper East Side-Carnegie Hill','MN50':'Stuyvesant Town-Cooper Village','MN99':'park-cemetery-etc-Manhattan',}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wknd</th>\n",
       "      <th>holiday</th>\n",
       "      <th>hours_to_resolution_stdz</th>\n",
       "      <th>AWND</th>\n",
       "      <th>PRCP</th>\n",
       "      <th>SNOW</th>\n",
       "      <th>SNWD</th>\n",
       "      <th>TMAX</th>\n",
       "      <th>WDF5</th>\n",
       "      <th>WSF5</th>\n",
       "      <th>WT01</th>\n",
       "      <th>WT02</th>\n",
       "      <th>WT04</th>\n",
       "      <th>WT08</th>\n",
       "      <th>d-1_cnt</th>\n",
       "      <th>d-2_cnt</th>\n",
       "      <th>d-3_cnt</th>\n",
       "      <th>d-4_cnt</th>\n",
       "      <th>d-5_cnt</th>\n",
       "      <th>d-6_cnt</th>\n",
       "      <th>d-7_cnt</th>\n",
       "      <th>d-8_cnt</th>\n",
       "      <th>d-9_cnt</th>\n",
       "      <th>d-10_cnt</th>\n",
       "      <th>d-11_cnt</th>\n",
       "      <th>d-12_cnt</th>\n",
       "      <th>d-13_cnt</th>\n",
       "      <th>d-14_cnt</th>\n",
       "      <th>d-15_cnt</th>\n",
       "      <th>d-16_cnt</th>\n",
       "      <th>d-17_cnt</th>\n",
       "      <th>d-18_cnt</th>\n",
       "      <th>d-19_cnt</th>\n",
       "      <th>d-20_cnt</th>\n",
       "      <th>d-21_cnt</th>\n",
       "      <th>d-22_cnt</th>\n",
       "      <th>d-23_cnt</th>\n",
       "      <th>d-24_cnt</th>\n",
       "      <th>d-25_cnt</th>\n",
       "      <th>d-26_cnt</th>\n",
       "      <th>d-27_cnt</th>\n",
       "      <th>d-28_cnt</th>\n",
       "      <th>ahv_open_cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.579233</td>\n",
       "      <td>-0.150566</td>\n",
       "      <td>-0.15404</td>\n",
       "      <td>0.107604</td>\n",
       "      <td>0.207273</td>\n",
       "      <td>-0.093056</td>\n",
       "      <td>7.447692</td>\n",
       "      <td>-1.20886</td>\n",
       "      <td>0.236286</td>\n",
       "      <td>1.105861</td>\n",
       "      <td>1.448874</td>\n",
       "      <td>5.77986</td>\n",
       "      <td>-0.04969</td>\n",
       "      <td>1.93755</td>\n",
       "      <td>-0.054409</td>\n",
       "      <td>-0.678413</td>\n",
       "      <td>-0.365704</td>\n",
       "      <td>-0.052903</td>\n",
       "      <td>-0.364801</td>\n",
       "      <td>-0.676902</td>\n",
       "      <td>-0.676595</td>\n",
       "      <td>-0.051068</td>\n",
       "      <td>-0.362986</td>\n",
       "      <td>-0.050179</td>\n",
       "      <td>-0.362176</td>\n",
       "      <td>-0.674027</td>\n",
       "      <td>-0.673530</td>\n",
       "      <td>-0.673262</td>\n",
       "      <td>-0.672701</td>\n",
       "      <td>-0.046703</td>\n",
       "      <td>-0.359266</td>\n",
       "      <td>-0.671618</td>\n",
       "      <td>-0.671181</td>\n",
       "      <td>-0.357729</td>\n",
       "      <td>-0.357459</td>\n",
       "      <td>-0.670058</td>\n",
       "      <td>-0.669689</td>\n",
       "      <td>-0.355967</td>\n",
       "      <td>-0.355679</td>\n",
       "      <td>-0.668450</td>\n",
       "      <td>-0.668321</td>\n",
       "      <td>-0.668131</td>\n",
       "      <td>-0.50966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.579233</td>\n",
       "      <td>-0.150566</td>\n",
       "      <td>-0.15404</td>\n",
       "      <td>0.107604</td>\n",
       "      <td>0.207273</td>\n",
       "      <td>-0.093056</td>\n",
       "      <td>7.447692</td>\n",
       "      <td>-1.20886</td>\n",
       "      <td>0.236286</td>\n",
       "      <td>1.105861</td>\n",
       "      <td>1.448874</td>\n",
       "      <td>5.77986</td>\n",
       "      <td>-0.04969</td>\n",
       "      <td>1.93755</td>\n",
       "      <td>0.570154</td>\n",
       "      <td>-0.678413</td>\n",
       "      <td>-0.365704</td>\n",
       "      <td>-0.052903</td>\n",
       "      <td>-0.052266</td>\n",
       "      <td>-0.364435</td>\n",
       "      <td>-0.364098</td>\n",
       "      <td>-0.051068</td>\n",
       "      <td>1.199736</td>\n",
       "      <td>-0.675180</td>\n",
       "      <td>2.450534</td>\n",
       "      <td>-0.674027</td>\n",
       "      <td>-0.673530</td>\n",
       "      <td>-0.673262</td>\n",
       "      <td>-0.672701</td>\n",
       "      <td>-0.046703</td>\n",
       "      <td>0.266390</td>\n",
       "      <td>-0.671618</td>\n",
       "      <td>-0.358164</td>\n",
       "      <td>-0.044646</td>\n",
       "      <td>-0.044333</td>\n",
       "      <td>-0.356905</td>\n",
       "      <td>1.209269</td>\n",
       "      <td>-0.669142</td>\n",
       "      <td>-0.355679</td>\n",
       "      <td>-0.668450</td>\n",
       "      <td>-0.041359</td>\n",
       "      <td>-0.668131</td>\n",
       "      <td>-0.50966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.579233</td>\n",
       "      <td>-0.150566</td>\n",
       "      <td>-0.15404</td>\n",
       "      <td>0.107604</td>\n",
       "      <td>0.207273</td>\n",
       "      <td>-0.093056</td>\n",
       "      <td>7.447692</td>\n",
       "      <td>-1.20886</td>\n",
       "      <td>0.236286</td>\n",
       "      <td>1.105861</td>\n",
       "      <td>1.448874</td>\n",
       "      <td>5.77986</td>\n",
       "      <td>-0.04969</td>\n",
       "      <td>1.93755</td>\n",
       "      <td>-0.678973</td>\n",
       "      <td>-0.366098</td>\n",
       "      <td>-0.053309</td>\n",
       "      <td>0.259503</td>\n",
       "      <td>-0.052266</td>\n",
       "      <td>-0.676902</td>\n",
       "      <td>-0.676595</td>\n",
       "      <td>-0.676091</td>\n",
       "      <td>-0.675530</td>\n",
       "      <td>-0.675180</td>\n",
       "      <td>-0.674699</td>\n",
       "      <td>-0.674027</td>\n",
       "      <td>-0.673530</td>\n",
       "      <td>-0.673262</td>\n",
       "      <td>-0.672701</td>\n",
       "      <td>-0.672357</td>\n",
       "      <td>-0.359266</td>\n",
       "      <td>-0.358690</td>\n",
       "      <td>-0.671181</td>\n",
       "      <td>-0.670812</td>\n",
       "      <td>-0.357459</td>\n",
       "      <td>-0.356905</td>\n",
       "      <td>-0.669689</td>\n",
       "      <td>-0.669142</td>\n",
       "      <td>-0.668918</td>\n",
       "      <td>-0.355143</td>\n",
       "      <td>-0.354840</td>\n",
       "      <td>-0.354589</td>\n",
       "      <td>-0.50966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.579233</td>\n",
       "      <td>-0.150566</td>\n",
       "      <td>-0.15404</td>\n",
       "      <td>0.107604</td>\n",
       "      <td>0.207273</td>\n",
       "      <td>-0.093056</td>\n",
       "      <td>7.447692</td>\n",
       "      <td>-1.20886</td>\n",
       "      <td>0.236286</td>\n",
       "      <td>1.105861</td>\n",
       "      <td>1.448874</td>\n",
       "      <td>5.77986</td>\n",
       "      <td>-0.04969</td>\n",
       "      <td>1.93755</td>\n",
       "      <td>-0.054409</td>\n",
       "      <td>-0.053784</td>\n",
       "      <td>-0.365704</td>\n",
       "      <td>-0.365308</td>\n",
       "      <td>-0.052266</td>\n",
       "      <td>-0.364435</td>\n",
       "      <td>-0.676595</td>\n",
       "      <td>-0.363579</td>\n",
       "      <td>-0.362986</td>\n",
       "      <td>-0.675180</td>\n",
       "      <td>-0.362176</td>\n",
       "      <td>-0.674027</td>\n",
       "      <td>-0.360913</td>\n",
       "      <td>-0.047878</td>\n",
       "      <td>-0.672701</td>\n",
       "      <td>-0.359530</td>\n",
       "      <td>-0.046438</td>\n",
       "      <td>-0.671618</td>\n",
       "      <td>0.267869</td>\n",
       "      <td>-0.044646</td>\n",
       "      <td>0.268793</td>\n",
       "      <td>-0.356905</td>\n",
       "      <td>0.582949</td>\n",
       "      <td>-0.042792</td>\n",
       "      <td>-0.355679</td>\n",
       "      <td>-0.668450</td>\n",
       "      <td>-0.668321</td>\n",
       "      <td>-0.668131</td>\n",
       "      <td>-0.50966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.579233</td>\n",
       "      <td>-0.150566</td>\n",
       "      <td>-0.15404</td>\n",
       "      <td>0.107604</td>\n",
       "      <td>0.207273</td>\n",
       "      <td>-0.093056</td>\n",
       "      <td>7.447692</td>\n",
       "      <td>-1.20886</td>\n",
       "      <td>0.236286</td>\n",
       "      <td>1.105861</td>\n",
       "      <td>1.448874</td>\n",
       "      <td>5.77986</td>\n",
       "      <td>-0.04969</td>\n",
       "      <td>1.93755</td>\n",
       "      <td>-0.366691</td>\n",
       "      <td>-0.053784</td>\n",
       "      <td>-0.365704</td>\n",
       "      <td>0.571909</td>\n",
       "      <td>-0.052266</td>\n",
       "      <td>-0.676902</td>\n",
       "      <td>0.260895</td>\n",
       "      <td>-0.676091</td>\n",
       "      <td>-0.675530</td>\n",
       "      <td>2.449824</td>\n",
       "      <td>0.575394</td>\n",
       "      <td>0.576100</td>\n",
       "      <td>-0.048297</td>\n",
       "      <td>0.577505</td>\n",
       "      <td>-0.047187</td>\n",
       "      <td>-0.672357</td>\n",
       "      <td>-0.046438</td>\n",
       "      <td>-0.358690</td>\n",
       "      <td>-0.358164</td>\n",
       "      <td>-0.670812</td>\n",
       "      <td>-0.670585</td>\n",
       "      <td>0.582554</td>\n",
       "      <td>-0.669689</td>\n",
       "      <td>-0.669142</td>\n",
       "      <td>-0.668918</td>\n",
       "      <td>-0.668450</td>\n",
       "      <td>-0.668321</td>\n",
       "      <td>-0.041047</td>\n",
       "      <td>-0.50966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       wknd   holiday  hours_to_resolution_stdz      AWND      PRCP      SNOW      SNWD     TMAX      WDF5      WSF5      WT01     WT02     WT04     WT08   d-1_cnt   d-2_cnt   d-3_cnt   d-4_cnt   d-5_cnt   d-6_cnt   d-7_cnt   d-8_cnt   d-9_cnt  d-10_cnt  d-11_cnt  d-12_cnt  d-13_cnt  d-14_cnt  d-15_cnt  d-16_cnt  d-17_cnt  d-18_cnt  d-19_cnt  d-20_cnt  d-21_cnt  d-22_cnt  d-23_cnt  d-24_cnt  d-25_cnt  d-26_cnt  d-27_cnt  d-28_cnt  ahv_open_cnt\n",
       "0  1.579233 -0.150566                  -0.15404  0.107604  0.207273 -0.093056  7.447692 -1.20886  0.236286  1.105861  1.448874  5.77986 -0.04969  1.93755 -0.054409 -0.678413 -0.365704 -0.052903 -0.364801 -0.676902 -0.676595 -0.051068 -0.362986 -0.050179 -0.362176 -0.674027 -0.673530 -0.673262 -0.672701 -0.046703 -0.359266 -0.671618 -0.671181 -0.357729 -0.357459 -0.670058 -0.669689 -0.355967 -0.355679 -0.668450 -0.668321 -0.668131      -0.50966\n",
       "1  1.579233 -0.150566                  -0.15404  0.107604  0.207273 -0.093056  7.447692 -1.20886  0.236286  1.105861  1.448874  5.77986 -0.04969  1.93755  0.570154 -0.678413 -0.365704 -0.052903 -0.052266 -0.364435 -0.364098 -0.051068  1.199736 -0.675180  2.450534 -0.674027 -0.673530 -0.673262 -0.672701 -0.046703  0.266390 -0.671618 -0.358164 -0.044646 -0.044333 -0.356905  1.209269 -0.669142 -0.355679 -0.668450 -0.041359 -0.668131      -0.50966\n",
       "2  1.579233 -0.150566                  -0.15404  0.107604  0.207273 -0.093056  7.447692 -1.20886  0.236286  1.105861  1.448874  5.77986 -0.04969  1.93755 -0.678973 -0.366098 -0.053309  0.259503 -0.052266 -0.676902 -0.676595 -0.676091 -0.675530 -0.675180 -0.674699 -0.674027 -0.673530 -0.673262 -0.672701 -0.672357 -0.359266 -0.358690 -0.671181 -0.670812 -0.357459 -0.356905 -0.669689 -0.669142 -0.668918 -0.355143 -0.354840 -0.354589      -0.50966\n",
       "3  1.579233 -0.150566                  -0.15404  0.107604  0.207273 -0.093056  7.447692 -1.20886  0.236286  1.105861  1.448874  5.77986 -0.04969  1.93755 -0.054409 -0.053784 -0.365704 -0.365308 -0.052266 -0.364435 -0.676595 -0.363579 -0.362986 -0.675180 -0.362176 -0.674027 -0.360913 -0.047878 -0.672701 -0.359530 -0.046438 -0.671618  0.267869 -0.044646  0.268793 -0.356905  0.582949 -0.042792 -0.355679 -0.668450 -0.668321 -0.668131      -0.50966\n",
       "4  1.579233 -0.150566                  -0.15404  0.107604  0.207273 -0.093056  7.447692 -1.20886  0.236286  1.105861  1.448874  5.77986 -0.04969  1.93755 -0.366691 -0.053784 -0.365704  0.571909 -0.052266 -0.676902  0.260895 -0.676091 -0.675530  2.449824  0.575394  0.576100 -0.048297  0.577505 -0.047187 -0.672357 -0.046438 -0.358690 -0.358164 -0.670812 -0.670585  0.582554 -0.669689 -0.669142 -0.668918 -0.668450 -0.668321 -0.041047      -0.50966"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_stdz = pd.read_csv('./features_stdz.csv')\n",
    "features_stdz.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "targets_cpu = np.loadtxt('./targets.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  3.,  0.,  2.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  2.,\n",
       "        0.,  0.,  0.,  3.,  1.,  0.,  0.,  1.,  1.,  1.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_cpu[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Inspect Hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "I am using MSELoss. Below shows that covariance matrix is essentially the Hessian of the Loss function. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{MSE} &= \\frac{1}{N}||W\\vec{x} - \\vec{y}||^2 \\quad \\hat{y} :=W\\vec{x}\\\\\n",
    "\\nabla_{W} \\text{MSE} &= \\frac{2}{N} ||W\\vec{x} - \\vec{y}||\\vec{x}^T \\\\\n",
    "\\nabla_{W}^2 \\text{MSE} &= \\frac{1}{N} 2\\vec{x}\\vec{x}^T \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Let us examine the covariance matrix of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2b107c0c2fd0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b107bbe7f28>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.clf()\n",
    "plt.imshow(features_stdz.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If the covariance matrix diverges from Identity, some eigenvectors may have a small magnitude, collapsing the corresponding axis of the Loss space. But, it's okay if you use `SGD` with a small learning rate, because you cannot examine the entire Hypothesis space. If repeated predictions result in similar minimization, we can be confident that it is close tothe global minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Measure sizes\n",
    "features = np.array(features_stdz)\n",
    "S = 29\n",
    "T = int(features.shape[0] / S)\n",
    "Fe=features.shape[1]\n",
    "H=Fe*S//4\n",
    "batch_size=1\n",
    "num_epochs = 40\n",
    "\n",
    "features_nn = features.reshape(T, S*Fe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1624, 1247), (406, 1247), (1624, 29), (406, 29))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features_nn\n",
    "    , targets_cpu\n",
    "    , test_size=0.2\n",
    "    , shuffle=False\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dev_cnt = torch.cuda.device_count()\n",
    "\n",
    "train_ds = DataLoader(\n",
    "    torch.from_numpy(np.concatenate((X_train, y_train), axis=1))\n",
    "    , batch_size=batch_size\n",
    "    , drop_last=True\n",
    "    , shuffle=True\n",
    "    , num_workers=dev_cnt*6\n",
    "    , pin_memory=True\n",
    ")\n",
    "\n",
    "test_ds = DataLoader(\n",
    "    torch.from_numpy(np.concatenate((X_test, y_test), axis=1))\n",
    "    , batch_size=batch_size\n",
    "    , drop_last=True    \n",
    "    , num_workers=dev_cnt*6\n",
    "    , pin_memory=True    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Here we define our model as a class\n",
    "class ManhattanModel(nn.Module):\n",
    "    def __init__(self, input_dim, feature_dim, hidden_dim, output_dim, num_layers, batch_size):\n",
    "        \"\"\"\n",
    "        The model uses LSTM model as both Encoder/Decoder for this undercomplete Autoencoder model.\n",
    "        * Batch normalization is used for all linear layers.\n",
    "        * The autoencoder compresses the representation to hidden_dim/4, \n",
    "            and then recovers the dimensionality back to hidden_dim\n",
    "        * Softmax layer is used to output pseudo-probability density of complaint volume \n",
    "            of each spatial bin on each day.\n",
    "        \"\"\"\n",
    "        super(ManhattanModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.feature_dim = feature_dim        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim        \n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm1 = nn.LSTM(\n",
    "            input_dim*feature_dim\n",
    "            , self.hidden_dim\n",
    "            , self.num_layers\n",
    "            , dropout=0.1\n",
    "            , batch_first=True\n",
    "        )\n",
    "        self.batchnorm1d_1 = nn.BatchNorm1d(batch_size)            \n",
    "        self.linear_1 = nn.Linear(self.hidden_dim, int(self.hidden_dim/2))\n",
    "        self.batchnorm1d_2 = nn.BatchNorm1d(batch_size)   \n",
    "        self.linear_2 = nn.Linear(int(self.hidden_dim/2), int(self.hidden_dim/4))\n",
    "        self.lstm2 = nn.LSTM(\n",
    "            int(self.hidden_dim/4)\n",
    "            , self.hidden_dim\n",
    "            , self.num_layers\n",
    "            , dropout=0.1\n",
    "            , batch_first=True\n",
    "        )\n",
    "        self.batchnorm1d_3 = nn.BatchNorm1d(batch_size)    \n",
    "        self.linear_3 = nn.Linear(self.hidden_dim, int(self.hidden_dim/2))\n",
    "        self.batchnorm1d_4 = nn.BatchNorm1d(batch_size)    \n",
    "        self.linear_4 = nn.Linear(int(self.hidden_dim/2), output_dim)        \n",
    "        \n",
    "    def init_hidden(self):\n",
    "        # This is what we'll initialise our hidden state as\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "\n",
    "    def forward(self, x, h1, h2):\n",
    "        if h1 is None:\n",
    "            x, h1 = self.lstm1(x)\n",
    "        else:\n",
    "            x, h1 = self.lstm1(x, h1)\n",
    "        x = self.batchnorm1d_1(x)\n",
    "        x = self.linear_1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.batchnorm1d_2(x)\n",
    "        x = self.linear_2(x)   \n",
    "        x = F.relu(x)        \n",
    "        if h2 is None:\n",
    "            x, h2 = self.lstm2(x)\n",
    "        else:\n",
    "            x, h2 = self.lstm2(x, h2)\n",
    "        x = self.linear_3(x)  \n",
    "        x = self.batchnorm1d_3(x)\n",
    "        x = F.relu(x)        \n",
    "        x = self.batchnorm1d_4(x)\n",
    "        x = self.linear_4(x)  \n",
    "        x = F.relu(x)  \n",
    "        x = F.softmax(x, dim=-1)\n",
    "        return x, h1, h2\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shy256/pyenv/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model_s = ManhattanModel(S, Fe, H, S, 1, batch_size)\n",
    "# model_s = ManhattanDenseNet(S, Fe, H, S, 2, batch_size)\n",
    "\n",
    "model = nn.DataParallel(model_s, device_ids=range(dev_cnt)) #I want to use all available GPUs anyhow\n",
    "# model = model.cuda()\n",
    "model.to(device)\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=8, gamma=0.1)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "losses = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shy256/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[test] [b]:0 - [loss]:0.0124927004799\n",
      "[test] [b]:1 - [loss]:0.0267216581851\n",
      "[test] [b]:2 - [loss]:0.0273507181555\n",
      "[test] [b]:3 - [loss]:0.00980117078871\n",
      "[test] [b]:4 - [loss]:0.00948901008815\n",
      "[test] [b]:5 - [loss]:0.0115746073425\n",
      "[test] [b]:6 - [loss]:0.00290232221596\n",
      "[test] [b]:7 - [loss]:0.0243801716715\n",
      "[test] [b]:8 - [loss]:0.0351493507624\n",
      "[test] [b]:9 - [loss]:0.0341621488333\n",
      "[test] [b]:10 - [loss]:0.0218577198684\n",
      "[test] [b]:11 - [loss]:0.00701396213844\n",
      "[test] [b]:12 - [loss]:0.0185701455921\n",
      "[test] [b]:13 - [loss]:0.0102488696575\n",
      "[test] [b]:14 - [loss]:0.0293127130717\n",
      "[test] [b]:15 - [loss]:0.0352380909026\n",
      "[test] [b]:16 - [loss]:0.0200921948999\n",
      "[test] [b]:17 - [loss]:0.00512331863865\n",
      "[test] [b]:18 - [loss]:0.0100087095052\n",
      "[test] [b]:19 - [loss]:0.00175111589488\n",
      "[test] [b]:20 - [loss]:0.0033907557372\n",
      "[test] [b]:21 - [loss]:0.0102584287524\n",
      "[test] [b]:22 - [loss]:0.0299352221191\n",
      "[test] [b]:23 - [loss]:0.0252590794116\n",
      "[test] [b]:24 - [loss]:0.0184788480401\n",
      "[test] [b]:25 - [loss]:0.0356043353677\n",
      "[test] [b]:26 - [loss]:0.00880589336157\n",
      "[test] [b]:27 - [loss]:0.00195305177476\n",
      "[test] [b]:28 - [loss]:0.0354747027159\n",
      "[test] [b]:29 - [loss]:0.0159927289933\n",
      "[test] [b]:30 - [loss]:0.00652571907267\n",
      "[test] [b]:31 - [loss]:0.0210469290614\n",
      "[test] [b]:32 - [loss]:0.0186294652522\n",
      "[test] [b]:33 - [loss]:0.0188876390457\n",
      "[test] [b]:34 - [loss]:0.00158615713008\n",
      "[test] [b]:35 - [loss]:0.00681749917567\n",
      "[test] [b]:36 - [loss]:0.00285054906271\n",
      "[test] [b]:37 - [loss]:0.0311104692519\n",
      "[test] [b]:38 - [loss]:0.0240395851433\n",
      "[test] [b]:39 - [loss]:0.0123523678631\n",
      "[test] [b]:40 - [loss]:0.0259457882494\n",
      "[test] [b]:41 - [loss]:0.00699329469353\n",
      "[test] [b]:42 - [loss]:0.00341187021695\n",
      "[test] [b]:43 - [loss]:0.0146190542728\n",
      "[test] [b]:44 - [loss]:0.0251361113042\n",
      "[test] [b]:45 - [loss]:0.0126739833504\n",
      "[test] [b]:46 - [loss]:0.00352280773222\n",
      "[test] [b]:47 - [loss]:0.0194632895291\n",
      "[test] [b]:48 - [loss]:0.002003606176\n",
      "[test] [b]:49 - [loss]:0.0123183457181\n",
      "[test] [b]:50 - [loss]:0.00802681036294\n",
      "[test] [b]:51 - [loss]:0.0349825583398\n",
      "[test] [b]:52 - [loss]:0.0354082807899\n",
      "[test] [b]:53 - [loss]:0.0173604302108\n",
      "[test] [b]:54 - [loss]:0.0250512249768\n",
      "[test] [b]:55 - [loss]:0.00082159723388\n",
      "[test] [b]:56 - [loss]:0.0132888779044\n",
      "[test] [b]:57 - [loss]:0.017845261842\n",
      "[test] [b]:58 - [loss]:0.0201889872551\n",
      "[test] [b]:59 - [loss]:0.0368605852127\n",
      "[test] [b]:60 - [loss]:0.0294987317175\n",
      "[test] [b]:61 - [loss]:0.0163778755814\n",
      "[test] [b]:62 - [loss]:0.00779194198549\n",
      "[test] [b]:63 - [loss]:0.00599571364\n",
      "[test] [b]:64 - [loss]:0.0350485742092\n",
      "[test] [b]:65 - [loss]:0.0303497035056\n",
      "[test] [b]:66 - [loss]:0.0350819788873\n",
      "[test] [b]:67 - [loss]:0.0180483255535\n",
      "[test] [b]:68 - [loss]:0.00358042679727\n",
      "[test] [b]:69 - [loss]:0.0318866446614\n",
      "[test] [b]:70 - [loss]:0.0257918220013\n",
      "[test] [b]:71 - [loss]:0.0310984048992\n",
      "[test] [b]:72 - [loss]:0.0332988128066\n",
      "[test] [b]:73 - [loss]:0.004210899584\n",
      "[test] [b]:74 - [loss]:0.0263190530241\n",
      "[test] [b]:75 - [loss]:0.0294773411006\n",
      "[test] [b]:76 - [loss]:0.0051142340526\n",
      "[test] [b]:77 - [loss]:0.0227203909308\n",
      "[test] [b]:78 - [loss]:0.0315698757768\n",
      "[test] [b]:79 - [loss]:0.0321726724505\n",
      "[test] [b]:80 - [loss]:0.0193138830364\n",
      "[test] [b]:81 - [loss]:0.0094116423279\n",
      "[test] [b]:82 - [loss]:0.0198011882603\n",
      "[test] [b]:83 - [loss]:0.00466937851161\n",
      "[test] [b]:84 - [loss]:0.00254696165211\n",
      "[test] [b]:85 - [loss]:0.0351974479854\n",
      "[test] [b]:86 - [loss]:0.0320471189916\n",
      "[test] [b]:87 - [loss]:0.0216994434595\n",
      "[test] [b]:88 - [loss]:0.00865968130529\n",
      "[test] [b]:89 - [loss]:0.0263689476997\n",
      "[test] [b]:90 - [loss]:0.0294630490243\n",
      "[test] [b]:91 - [loss]:0.0155684221536\n",
      "[test] [b]:92 - [loss]:0.03324213624\n",
      "[test] [b]:93 - [loss]:0.0244715698063\n",
      "[test] [b]:94 - [loss]:0.0101173790172\n",
      "[test] [b]:95 - [loss]:0.018353600055\n",
      "[test] [b]:96 - [loss]:0.0357256270945\n",
      "[test] [b]:97 - [loss]:0.0320234037936\n",
      "[test] [b]:98 - [loss]:0.0166558474302\n",
      "[test] [b]:99 - [loss]:0.0232978314161\n",
      "[test] [b]:100 - [loss]:0.0164473094046\n",
      "[test] [b]:101 - [loss]:0.00320006487891\n",
      "[test] [b]:102 - [loss]:0.0110976863652\n",
      "[test] [b]:103 - [loss]:0.012000657618\n",
      "[test] [b]:104 - [loss]:0.0164703354239\n",
      "[test] [b]:105 - [loss]:0.0303104445338\n",
      "[test] [b]:106 - [loss]:0.0141486525536\n",
      "[test] [b]:107 - [loss]:0.0350893437862\n",
      "[test] [b]:108 - [loss]:0.0383164845407\n",
      "[test] [b]:109 - [loss]:0.0326426587999\n",
      "[test] [b]:110 - [loss]:0.0365794897079\n",
      "[test] [b]:111 - [loss]:0.0316375233233\n",
      "[test] [b]:112 - [loss]:0.0129234362394\n",
      "[test] [b]:113 - [loss]:0.0335704237223\n",
      "[test] [b]:114 - [loss]:0.0227297469974\n",
      "[test] [b]:115 - [loss]:0.0294814258814\n",
      "[test] [b]:116 - [loss]:0.0270242262632\n",
      "[test] [b]:117 - [loss]:0.0144931515679\n",
      "[test] [b]:118 - [loss]:0.0312992446125\n",
      "[test] [b]:119 - [loss]:0.0367886349559\n",
      "[test] [b]:120 - [loss]:0.0362163707614\n",
      "[test] [b]:121 - [loss]:0.0366949141026\n",
      "[test] [b]:122 - [loss]:0.0369208008051\n",
      "[test] [b]:123 - [loss]:0.0168901607394\n",
      "[test] [b]:124 - [loss]:0.00980613660067\n",
      "[test] [b]:125 - [loss]:0.0266741309315\n",
      "[test] [b]:126 - [loss]:0.00240914500318\n",
      "[test] [b]:127 - [loss]:0.0373606830835\n",
      "[test] [b]:128 - [loss]:0.00663660652936\n",
      "[test] [b]:129 - [loss]:0.00940679572523\n",
      "[test] [b]:130 - [loss]:0.0122373327613\n",
      "[test] [b]:131 - [loss]:0.0220791269094\n",
      "[test] [b]:132 - [loss]:0.0361324101686\n",
      "[test] [b]:133 - [loss]:0.0116470716894\n",
      "[test] [b]:134 - [loss]:0.0102220317349\n",
      "[test] [b]:135 - [loss]:0.0322149321437\n",
      "[test] [b]:136 - [loss]:0.00962026044726\n",
      "[test] [b]:137 - [loss]:0.0342367254198\n",
      "[test] [b]:138 - [loss]:0.0367984734476\n",
      "[test] [b]:139 - [loss]:0.0250058956444\n",
      "[test] [b]:140 - [loss]:0.0238517764956\n",
      "[test] [b]:141 - [loss]:0.0338876321912\n",
      "[test] [b]:142 - [loss]:0.0237385686487\n",
      "[test] [b]:143 - [loss]:0.0207017250359\n",
      "[test] [b]:144 - [loss]:0.00316915707663\n",
      "[test] [b]:145 - [loss]:0.0352598540485\n",
      "[test] [b]:146 - [loss]:0.0116254696622\n",
      "[test] [b]:147 - [loss]:0.0158305615187\n",
      "[test] [b]:148 - [loss]:0.0362424477935\n",
      "[test] [b]:149 - [loss]:0.0219110492617\n",
      "[test] [b]:150 - [loss]:0.0355420783162\n",
      "[test] [b]:151 - [loss]:0.0278668422252\n",
      "[test] [b]:152 - [loss]:0.00894429162145\n",
      "[test] [b]:153 - [loss]:0.0070190615952\n",
      "[test] [b]:154 - [loss]:0.0238704234362\n",
      "[test] [b]:155 - [loss]:0.0353547558188\n",
      "[test] [b]:156 - [loss]:0.0347255952656\n",
      "[test] [b]:157 - [loss]:0.0162451975048\n",
      "[test] [b]:158 - [loss]:0.00364649738185\n",
      "[test] [b]:159 - [loss]:0.00156391737983\n",
      "[test] [b]:160 - [loss]:0.00213769823313\n",
      "[test] [b]:161 - [loss]:0.0027338557411\n",
      "[test] [b]:162 - [loss]:0.0312987677753\n",
      "[test] [b]:163 - [loss]:0.0129885571077\n",
      "[test] [b]:164 - [loss]:0.00427146675065\n",
      "[test] [b]:165 - [loss]:0.0290901418775\n",
      "[test] [b]:166 - [loss]:0.00330031407066\n",
      "[test] [b]:167 - [loss]:0.0087752956897\n",
      "[test] [b]:168 - [loss]:0.0232473183423\n",
      "[test] [b]:169 - [loss]:0.0353737585247\n",
      "[test] [b]:170 - [loss]:0.0354089252651\n",
      "[test] [b]:171 - [loss]:0.035473678261\n",
      "[test] [b]:172 - [loss]:0.0355002880096\n",
      "[test] [b]:173 - [loss]:0.0032324437052\n",
      "[test] [b]:174 - [loss]:0.0172865614295\n",
      "[test] [b]:175 - [loss]:0.018493976444\n",
      "[test] [b]:176 - [loss]:0.0180396754295\n",
      "[test] [b]:177 - [loss]:0.00779824890196\n",
      "[test] [b]:178 - [loss]:0.0290563572198\n",
      "[test] [b]:179 - [loss]:0.036184836179\n",
      "[test] [b]:180 - [loss]:0.0158015843481\n",
      "[test] [b]:181 - [loss]:0.00222844793461\n",
      "[test] [b]:182 - [loss]:0.0199186578393\n",
      "[test] [b]:183 - [loss]:0.0313368700445\n",
      "[test] [b]:184 - [loss]:0.0153407072648\n",
      "[test] [b]:185 - [loss]:0.0276878308505\n",
      "[test] [b]:186 - [loss]:0.0309342611581\n",
      "[test] [b]:187 - [loss]:0.00695517193526\n",
      "[test] [b]:188 - [loss]:0.023558402434\n",
      "[test] [b]:189 - [loss]:0.0313440412283\n",
      "[test] [b]:190 - [loss]:0.0175308194011\n",
      "[test] [b]:191 - [loss]:0.0122620137408\n",
      "[test] [b]:192 - [loss]:0.0319317616522\n",
      "[test] [b]:193 - [loss]:0.0197341516614\n",
      "[test] [b]:194 - [loss]:0.0347687415779\n",
      "[test] [b]:195 - [loss]:0.0241027586162\n",
      "[test] [b]:196 - [loss]:0.00955243688077\n",
      "[test] [b]:197 - [loss]:0.0264684781432\n",
      "[test] [b]:198 - [loss]:0.0356241986156\n",
      "[test] [b]:199 - [loss]:0.0137240076438\n",
      "[test] [b]:200 - [loss]:0.00831514038146\n",
      "[test] [b]:201 - [loss]:0.0310884136707\n",
      "[test] [b]:202 - [loss]:0.0282011311501\n",
      "[test] [b]:203 - [loss]:0.0162267461419\n",
      "[test] [b]:204 - [loss]:0.0331652536988\n",
      "[test] [b]:205 - [loss]:0.0301946680993\n",
      "[test] [b]:206 - [loss]:0.0309524890035\n",
      "[test] [b]:207 - [loss]:0.0341086685658\n",
      "[test] [b]:208 - [loss]:0.00420069647953\n",
      "[test] [b]:209 - [loss]:0.0338065363467\n",
      "[test] [b]:210 - [loss]:0.0305381566286\n",
      "[test] [b]:211 - [loss]:0.0200652554631\n",
      "[test] [b]:212 - [loss]:0.0107274213806\n",
      "[test] [b]:213 - [loss]:0.00775145366788\n",
      "[test] [b]:214 - [loss]:0.0221251491457\n",
      "[test] [b]:215 - [loss]:0.0176341421902\n",
      "[test] [b]:216 - [loss]:0.0186335369945\n",
      "[test] [b]:217 - [loss]:0.00589470006526\n",
      "[test] [b]:218 - [loss]:0.0325782783329\n",
      "[test] [b]:219 - [loss]:0.0323273576796\n",
      "[test] [b]:220 - [loss]:0.0182903371751\n",
      "[test] [b]:221 - [loss]:0.00550633249804\n",
      "[test] [b]:222 - [loss]:0.0131878582761\n",
      "[test] [b]:223 - [loss]:0.0214653443545\n",
      "[test] [b]:224 - [loss]:0.0337267778814\n",
      "[test] [b]:225 - [loss]:0.0355981551111\n",
      "[test] [b]:226 - [loss]:0.0334684960544\n",
      "[test] [b]:227 - [loss]:0.0175524875522\n",
      "[test] [b]:228 - [loss]:0.0157063826919\n",
      "[test] [b]:229 - [loss]:0.0225484520197\n",
      "[test] [b]:230 - [loss]:0.0166654735804\n",
      "[test] [b]:231 - [loss]:0.0309279635549\n",
      "[test] [b]:232 - [loss]:0.0336444862187\n",
      "[test] [b]:233 - [loss]:0.0102668972686\n",
      "[test] [b]:234 - [loss]:0.0242091417313\n",
      "[test] [b]:235 - [loss]:0.0247716363519\n",
      "[test] [b]:236 - [loss]:0.032806519419\n",
      "[test] [b]:237 - [loss]:0.00641955900937\n",
      "[test] [b]:238 - [loss]:0.0361960828304\n",
      "[test] [b]:239 - [loss]:0.00363936508074\n",
      "[test] [b]:240 - [loss]:0.0146848978475\n",
      "[test] [b]:241 - [loss]:0.0250680036843\n",
      "[test] [b]:242 - [loss]:0.0226804595441\n",
      "[test] [b]:243 - [loss]:0.0264912210405\n",
      "[test] [b]:244 - [loss]:0.00152016780339\n",
      "[test] [b]:245 - [loss]:0.00774825271219\n",
      "[test] [b]:246 - [loss]:0.0161911137402\n",
      "[test] [b]:247 - [loss]:0.0331285707653\n",
      "[test] [b]:248 - [loss]:0.0106898108497\n",
      "[test] [b]:249 - [loss]:0.0282280165702\n",
      "[test] [b]:250 - [loss]:0.0119416769594\n",
      "[test] [b]:251 - [loss]:0.0223671030253\n",
      "[test] [b]:252 - [loss]:0.0353105217218\n",
      "[test] [b]:253 - [loss]:0.035259835422\n",
      "[test] [b]:254 - [loss]:0.0352854728699\n",
      "[test] [b]:255 - [loss]:0.032254293561\n",
      "[test] [b]:256 - [loss]:0.0064121237956\n",
      "[test] [b]:257 - [loss]:0.00561518175527\n",
      "[test] [b]:258 - [loss]:0.0103140966967\n",
      "[test] [b]:259 - [loss]:0.0333631262183\n",
      "[test] [b]:260 - [loss]:0.0365816429257\n",
      "[test] [b]:261 - [loss]:0.0318503826857\n",
      "[test] [b]:262 - [loss]:0.0163628254086\n",
      "[test] [b]:263 - [loss]:0.0184125900269\n",
      "[test] [b]:264 - [loss]:0.00994114670902\n",
      "[test] [b]:265 - [loss]:0.0223692730069\n",
      "[test] [b]:266 - [loss]:0.0286086443812\n",
      "[test] [b]:267 - [loss]:0.0356829017401\n",
      "[test] [b]:268 - [loss]:0.0289120208472\n",
      "[test] [b]:269 - [loss]:0.0292169880122\n",
      "[test] [b]:270 - [loss]:0.0259448103607\n",
      "[test] [b]:271 - [loss]:0.015412430279\n",
      "[test] [b]:272 - [loss]:0.00395259167999\n",
      "[test] [b]:273 - [loss]:0.0186486709863\n",
      "[test] [b]:274 - [loss]:0.0147833041847\n",
      "[test] [b]:275 - [loss]:0.0132130654529\n",
      "[test] [b]:276 - [loss]:0.0100063579157\n",
      "[test] [b]:277 - [loss]:0.0144408103079\n",
      "[test] [b]:278 - [loss]:0.0120747666806\n",
      "[test] [b]:279 - [loss]:0.028063826263\n",
      "[test] [b]:280 - [loss]:0.0260787270963\n",
      "[test] [b]:281 - [loss]:0.0185337606817\n",
      "[test] [b]:282 - [loss]:0.0172911435366\n",
      "[test] [b]:283 - [loss]:0.0203209985048\n",
      "[test] [b]:284 - [loss]:0.0162728149444\n",
      "[test] [b]:285 - [loss]:0.0353389307857\n",
      "[test] [b]:286 - [loss]:0.0177254695445\n",
      "[test] [b]:287 - [loss]:0.00905563589185\n",
      "[test] [b]:288 - [loss]:0.0319385118783\n",
      "[test] [b]:289 - [loss]:0.0373706929386\n",
      "[test] [b]:290 - [loss]:0.0275817941874\n",
      "[test] [b]:291 - [loss]:0.0371538251638\n",
      "[test] [b]:292 - [loss]:0.0322877429426\n",
      "[test] [b]:293 - [loss]:0.0353708192706\n",
      "[test] [b]:294 - [loss]:0.0369871035218\n",
      "[test] [b]:295 - [loss]:0.0333240553737\n",
      "[test] [b]:296 - [loss]:0.0040811910294\n",
      "[test] [b]:297 - [loss]:0.00974841602147\n",
      "[test] [b]:298 - [loss]:0.0278484746814\n",
      "[test] [b]:299 - [loss]:0.0119553981349\n",
      "[test] [b]:300 - [loss]:0.018933063373\n",
      "[test] [b]:301 - [loss]:0.0135740516707\n",
      "[test] [b]:302 - [loss]:0.0372841469944\n",
      "[test] [b]:303 - [loss]:0.0364149399102\n",
      "[test] [b]:304 - [loss]:0.0321646034718\n",
      "[test] [b]:305 - [loss]:0.0330691114068\n",
      "[test] [b]:306 - [loss]:0.0333417318761\n",
      "[test] [b]:307 - [loss]:0.0346713364124\n",
      "[test] [b]:308 - [loss]:0.00217345822603\n",
      "[test] [b]:309 - [loss]:0.0294662285596\n",
      "[test] [b]:310 - [loss]:0.0360967479646\n",
      "[test] [b]:311 - [loss]:0.0332672335207\n",
      "[test] [b]:312 - [loss]:0.00697524053976\n",
      "[test] [b]:313 - [loss]:0.0184853412211\n",
      "[test] [b]:314 - [loss]:0.020767653361\n",
      "[test] [b]:315 - [loss]:0.00457094749436\n",
      "[test] [b]:316 - [loss]:0.00474462844431\n",
      "[test] [b]:317 - [loss]:0.0203114990145\n",
      "[test] [b]:318 - [loss]:0.0352902747691\n",
      "[test] [b]:319 - [loss]:0.0361467637122\n",
      "[test] [b]:320 - [loss]:0.0138523140922\n",
      "[test] [b]:321 - [loss]:0.00642006052658\n",
      "[test] [b]:322 - [loss]:0.0241949502379\n",
      "[test] [b]:323 - [loss]:0.00811703223735\n",
      "[test] [b]:324 - [loss]:0.0360780209303\n",
      "[test] [b]:325 - [loss]:0.026097657159\n",
      "[test] [b]:326 - [loss]:0.0302126295865\n",
      "[test] [b]:327 - [loss]:0.0261284150183\n",
      "[test] [b]:328 - [loss]:0.0028749704361\n",
      "[test] [b]:329 - [loss]:0.0198917835951\n",
      "[test] [b]:330 - [loss]:0.0181336849928\n",
      "[test] [b]:331 - [loss]:0.031888499856\n",
      "[test] [b]:332 - [loss]:0.005991905462\n",
      "[test] [b]:333 - [loss]:0.0170515030622\n",
      "[test] [b]:334 - [loss]:0.0316262021661\n",
      "[test] [b]:335 - [loss]:0.00191171793267\n",
      "[test] [b]:336 - [loss]:0.0299920961261\n",
      "[test] [b]:337 - [loss]:0.018202861771\n",
      "[test] [b]:338 - [loss]:0.0134442802519\n",
      "[test] [b]:339 - [loss]:0.0313583984971\n",
      "[test] [b]:340 - [loss]:0.0322766453028\n",
      "[test] [b]:341 - [loss]:0.0208613481373\n",
      "[test] [b]:342 - [loss]:0.00903368834406\n",
      "[test] [b]:343 - [loss]:0.0276977103204\n",
      "[test] [b]:344 - [loss]:0.0169739611447\n",
      "[test] [b]:345 - [loss]:0.0111822392792\n",
      "[test] [b]:346 - [loss]:0.0204529836774\n",
      "[test] [b]:347 - [loss]:0.00411878805608\n",
      "[test] [b]:348 - [loss]:0.0132099008188\n",
      "[test] [b]:349 - [loss]:0.00791479740292\n",
      "[test] [b]:350 - [loss]:0.0315862521529\n",
      "[test] [b]:351 - [loss]:0.00497513124719\n",
      "[test] [b]:352 - [loss]:0.00646811304614\n",
      "[test] [b]:353 - [loss]:0.0138253429905\n",
      "[test] [b]:354 - [loss]:0.0142120011151\n",
      "[test] [b]:355 - [loss]:0.00183873635251\n",
      "[test] [b]:356 - [loss]:0.00537448190153\n",
      "[test] [b]:357 - [loss]:0.00890614278615\n",
      "[test] [b]:358 - [loss]:0.0312312636524\n",
      "[test] [b]:359 - [loss]:0.0315930321813\n",
      "[test] [b]:360 - [loss]:0.0310183502734\n",
      "[test] [b]:361 - [loss]:0.00185556546785\n",
      "[test] [b]:362 - [loss]:0.00310784974135\n",
      "[test] [b]:363 - [loss]:0.00174402608536\n",
      "[test] [b]:364 - [loss]:0.0205406229943\n",
      "[test] [b]:365 - [loss]:0.0069397976622\n",
      "[test] [b]:366 - [loss]:0.0363705307245\n",
      "[test] [b]:367 - [loss]:0.034038785845\n",
      "[test] [b]:368 - [loss]:0.0020502328407\n",
      "[test] [b]:369 - [loss]:0.00911032874137\n",
      "[test] [b]:370 - [loss]:0.00638284208253\n",
      "[test] [b]:371 - [loss]:0.0148045206442\n",
      "[test] [b]:372 - [loss]:0.0276480633765\n",
      "[test] [b]:373 - [loss]:0.0266356430948\n",
      "[test] [b]:374 - [loss]:0.0251961220056\n",
      "[test] [b]:375 - [loss]:0.0278584361076\n",
      "[test] [b]:376 - [loss]:0.0333553850651\n",
      "[test] [b]:377 - [loss]:0.0344552434981\n",
      "[test] [b]:378 - [loss]:0.0354079827666\n",
      "[test] [b]:379 - [loss]:0.035486087203\n",
      "[test] [b]:380 - [loss]:0.0354877561331\n",
      "[test] [b]:381 - [loss]:0.0358420535922\n",
      "[test] [b]:382 - [loss]:0.0138879688457\n",
      "[test] [b]:383 - [loss]:0.00573894428089\n",
      "[test] [b]:384 - [loss]:0.00449817581102\n",
      "[test] [b]:385 - [loss]:0.0199099313468\n",
      "[test] [b]:386 - [loss]:0.0128621272743\n",
      "[test] [b]:387 - [loss]:0.0245017819107\n",
      "[test] [b]:388 - [loss]:0.00858268607408\n",
      "[test] [b]:389 - [loss]:0.0124805942178\n",
      "[test] [b]:390 - [loss]:0.0115627581254\n",
      "[test] [b]:391 - [loss]:0.00832350552082\n",
      "[test] [b]:392 - [loss]:0.0176354255527\n",
      "[test] [b]:393 - [loss]:0.0136209223419\n",
      "[test] [b]:394 - [loss]:0.0255527179688\n",
      "[test] [b]:395 - [loss]:0.00574982259423\n",
      "[test] [b]:396 - [loss]:0.0117270322517\n",
      "[test] [b]:397 - [loss]:0.00602072058246\n",
      "[test] [b]:398 - [loss]:0.00629892433062\n",
      "[test] [b]:399 - [loss]:0.00268611079082\n",
      "[test] [b]:400 - [loss]:0.00638060411438\n",
      "[test] [b]:401 - [loss]:0.0161624439061\n",
      "[test] [b]:402 - [loss]:0.0302640013397\n",
      "[test] [b]:403 - [loss]:0.0256101097912\n",
      "[test] [b]:404 - [loss]:0.0200564488769\n",
      "[test] [b]:405 - [loss]:0.0104597825557\n"
     ]
    }
   ],
   "source": [
    "d = torch.load('/scratch/shy256/capstone/cs_lstm_L1_AEDE_SM.pth')\n",
    "\n",
    "model.load_state_dict(d)\n",
    "\n",
    "model.eval()\n",
    "losses = []\n",
    "preds = []\n",
    "targets = []\n",
    "c_1, c_2 = None, None\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, data in enumerate(test_ds):\n",
    "        X_test_bf = data[:, :Fe*S].reshape(1, batch_size, Fe*S).type(torch.cuda.FloatTensor)\n",
    "        y_test_bf = data[:, Fe*S:].type(torch.cuda.FloatTensor)\n",
    "        output, c_1, c_2 = model(X_test_bf, c_1, c_2)\n",
    "        collapsed_size = torch.tensor(y_test_bf.shape).cumprod(dim=0)[-1].item()\n",
    "        pred = output.view(collapsed_size)\n",
    "        preds.extend(pred)\n",
    "        target = F.softmax(y_test_bf.view(collapsed_size))\n",
    "        targets.extend(target)        \n",
    "        loss = loss_fn(\n",
    "            pred.squeeze()\n",
    "            , target.squeeze()\n",
    "        )\n",
    "        losses.append(loss.item())\n",
    "        loss_t = np.mean(losses[-1:])\n",
    "        print(\n",
    "            '[test] [b]:%s - [loss]:%s' \\\n",
    "            % (batch_idx, str(loss_t))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "target_vals_orig = np.array(targets).reshape(len(targets) // 29, 29)\n",
    "pred_vals_orig = np.array(preds).reshape(len(targets) // 29, 29)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "target_vals = target_vals_orig.copy()\n",
    "pred_vals = pred_vals_orig.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "plt.clf()\n",
    "dts = pd.date_range(\n",
    "    datetime.datetime.strptime(\"2017-08-27\", '%Y-%m-%d') - relativedelta(days=len(losses) - 1)\n",
    "    , datetime.datetime.strptime(\"2017-08-27\", '%Y-%m-%d')\n",
    ")\n",
    "dts_str = [datetime.datetime.strftime(dt, '%Y-%m-%d') for dt in dts]\n",
    "mn_nms = [\"MN14\" , \"MN31\" , \"MN32\" , \"MN12\" , \"MN20\" , \"MN23\" , \"MN24\" , \"MN01\" , \"MN03\" , \"MN09\" , \"MN13\" , \"MN22\" , \"MN25\" , \"MN27\" , \"MN36\" , \"MN17\" , \"MN19\" , \"MN21\" , \"MN28\" , \"MN33\" , \"MN15\" , \"MN35\" , \"MN40\" , \"MN11\" , \"MN04\" , \"MN34\" , \"MN06\" , \"MN99\" , \"MN50\"]\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    S * 2 + 2, 1, figsize=(16, 80)\n",
    ")\n",
    "# plt.subplots_adjust(bottom=0.9, top=1)\n",
    "axs[0].plot(dts, target_vals.sum(axis=1), label='target')\n",
    "axs[0].plot(dts, pred_vals.sum(axis=1), label='prediction')\n",
    "tot_avg = target_vals.sum(axis=1).astype(np.float64).mean()\n",
    "tot_std = target_vals.sum(axis=1).astype(np.float64).std()\n",
    "axs[0].set_ylabel(f'Total\\nm:{tot_avg:.1f}\\nstdev:{tot_std:.1f}')\n",
    "# axs[1].plot(dts, losses)\n",
    "\n",
    "tot_acc = np.array(torch.Tensor((pred_vals.sum(axis=1) / target_vals.sum(axis=1)).astype(np.float64)).cpu())\n",
    "axs[1].plot(\n",
    "    dts, tot_acc * 100, color='g', label='score'    \n",
    ")\n",
    "axs[1].set_ylim(0, 100)\n",
    "axs[1].set_ylabel('Score') \n",
    "axs[1].bar(dts, tot_acc * 100, 1.1, color='g')\n",
    "axs[1].bar(dts, 100-tot_acc*100, 1.1, bottom=tot_acc * 100, color='r')\n",
    "\n",
    "losses = np.array(losses)\n",
    "loss_avg = losses.mean()\n",
    "loss_std = losses.std()\n",
    "axs[1].set_ylabel('Score')\n",
    "\n",
    "for i in range(0, S):\n",
    "    if i == 15:\n",
    "        break\n",
    "    # replace zero with the mean to make the prediction accuracy graph look better\n",
    "    index_both_zero = list(set(np.where(target_vals[:,i].astype(np.float64) == 0)[0]) & set(np.where(pred_vals[:,i].astype(np.float64) < 5e-1)[0]))\n",
    "    np.put(target_vals[:,i], index_both_zero, [1] * len(index_both_zero))\n",
    "    np.put(pred_vals[:,i], index_both_zero, [1] * len(index_both_zero))\n",
    "    # if target of pred is zero and the other is not, assign 0.5 to coerce to accuracy\n",
    "    target_vals[:,i][target_vals[:,i] <= 8e-1] = 1e-5\n",
    "    pred_vals[:,i][pred_vals[:,i] <= 8e-1] = 1e-5\n",
    "    denom = np.max(np.array([target_vals[:,i], pred_vals[:,i]]), axis=0)\n",
    "    numer = np.min(np.array([target_vals[:,i], pred_vals[:,i]]), axis=0)\n",
    "    acc = np.array(torch.Tensor((numer / denom).astype(np.float64)).cpu())\n",
    "    t, p, nm = target_vals[:,i].astype(np.float64), pred_vals[:,i].astype(np.float64), mn_nms[i]\n",
    "    i *= 2\n",
    "    i += 2    \n",
    "    axs[i].plot(\n",
    "        dts, t, 'r-'\n",
    "        , dts, p, 'b-'\n",
    "    )\n",
    "    cnt_avg = t.mean()\n",
    "    pred_cnt_avg = p.mean()    \n",
    "    cnt_std = t.std()    \n",
    "    pred_cnt_std = p.std()       \n",
    "    axs[i].set_ylabel(spatial_dict[nm].replace(' ', '\\n') + f'\\nm:{cnt_avg:.1f}/{pred_cnt_avg:.1f}\\nstdev:{cnt_std:.1f}/{pred_cnt_std:.1f}')\n",
    "    acc[~np.isfinite(acc)] = 0\n",
    "    axs[i + 1].plot(\n",
    "        dts, acc * 100, color='g', label='score'    \n",
    "    )\n",
    "    axs[i + 1].set_ylim(0, 100)\n",
    "    axs[i + 1].set_ylabel('Score') \n",
    "    axs[i + 1].bar(dts, acc * 100, 1.1, color='g')\n",
    "    axs[i + 1].bar(dts, 100-acc*100, 1.1, bottom=acc * 100, color='r')\n",
    "plt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.25,\n",
    "                    wspace=0.05)\n",
    "plt.savefig('./test_result_b1-shuf_manhattan_dn.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "target_vals = target_vals_orig.copy()\n",
    "pred_vals = pred_vals_orig.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# see if I can do better by using the aggregate ranking\n",
    "static_ranking = np.flip(\n",
    "    np.array(list([spatial_dict[k] for k in mn_nms]))[np.argsort(target_vals.sum(axis=0).astype(np.float64))], 0)[:15]\n",
    "static_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Check the frequentists of the Model's Dynamic Ranking\n",
    "a = []\n",
    "[a.extend(list(frozenset(np.flip(np.array(list([spatial_dict[k] for k in mn_nms]))[np.argsort(target_vals[II].astype(np.float64))], 0)[:15]))) for II in range(0, 200)]\n",
    "\n",
    "pd.Series(a).unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "A = []\n",
    "static_ranking_arr = []\n",
    "for i in range(406):\n",
    "    v = len(list(set(np.flip(np.array(list([spatial_dict[k] for k in mn_nms]))[np.argsort(target_vals[i].astype(np.float64))], 0)[:15]) & \\\n",
    "set(np.flip(np.array(list([spatial_dict[k] for k in mn_nms]))[np.argsort(pred_vals[i].astype(np.float64))], 0)[:15]))) / 15\n",
    "    A.append(v)\n",
    "    v = len(list(set(np.flip(np.array(list(spatial_dict.values()))[np.argsort(target_vals[i].astype(np.float64))], 0)[:15]) & \\\n",
    "set(static_ranking))) / 15\n",
    "    static_ranking_arr.append(v)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 4))\n",
    "A = np.array(A)\n",
    "axs[0].plot(A, color='#afeeee')\n",
    "model_mean = np.array(A).mean()\n",
    "axs[0].set_title(f'Prediction with Model\\nmean accuracy: {model_mean:.3f}')\n",
    "axs[0].set_ylim(0, 1)\n",
    "axs[0].set_xlabel('Days')\n",
    "axs[0].set_ylabel('Score')\n",
    "axs[0].bar(np.arange(A.size), A, 1.1, color='#afeeee')\n",
    "axs[0].bar(np.arange(A.size), 1-A, 1.1, bottom=A, color='magenta')\n",
    "\n",
    "\n",
    "\n",
    "static_ranking_arr = np.array(static_ranking_arr)\n",
    "axs[1].plot(static_ranking_arr, color='#afeeee')\n",
    "static_ranking_mean = np.array(static_ranking_arr).mean()\n",
    "axs[1].set_title(f'Prediction with Static Ranking\\nmean accuracy: {static_ranking_mean:.3f}')\n",
    "axs[1].set_ylim(0, 1)\n",
    "axs[1].set_xlabel('Days')\n",
    "axs[1].set_ylabel('Score')\n",
    "axs[1].bar(np.arange(static_ranking_arr.size), static_ranking_arr, 1.1, color='#afeeee')\n",
    "axs[1].bar(np.arange(static_ranking_arr.size), 1-static_ranking_arr, 1.1, bottom=static_ranking_arr, color='magenta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Bibliography\n",
    "\n",
    "Menne, M.J., I. Durre, R.S. Vose, B.E. Gleason, and T.G. Houston, 2012:  An overview \n",
    "of the Global Historical Climatology Network-Daily Database.  Journal of Atmospheric \n",
    "and Oceanic Technology, 29, 897-910, doi:10.1175/JTECH-D-11-00103.1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpc-PY363",
   "language": "python",
   "name": "py3.6.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
